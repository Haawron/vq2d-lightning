defaults:
  - _self_
  - augment: base
  - model: vqloc
  - override hydra/job_logging: none
  - override hydra/hydra_logging: none



# frequently overridden parameters
random_seed: 42
batch_size: 3
num_workers: 6
prefetch_factor: 4
base_lr: 0.0003  # 3e-4
compile: false
backbone_precision: 'fp32'  # bf16 or fp32 or fp16
backbone_fp32_mm_precision: 'highest'  # medium or high or highest


# runtime parameters
jid: ${oc.env:SLURM_JOB_ID}
job_type: ${job_type:}  # debug or batch
YY: ${now:%Y}
mm: ${now:%m}  # month
dd: ${now:%d}  # day
runtime_outdir: ${runtime_outdir:}
total_bsz: ${eval:'${dataset.batch_size} * ${oc.env:SLURM_GPUS_ON_NODE} * ${oc.env:SLURM_JOB_NUM_NODES,1}'}
max_steps: ${eval:'${trainer.max_epochs} * 13607 // ${total_bsz} + 1'}


run_type: train  # train, val, test, predict=eval


trainer:
  ##### directly affects to performance #####
  devices: 'auto'  # maximum available
  max_epochs: 106
  max_steps: ${max_steps}
  accumulate_grad_batches: 1
  precision: 32-true  # don't edit this, we autocast manually
  gradient_clip_val: 20.
  detect_anomaly: False
  benchmark: True
  deterministic: False  # sorry
  ##### logging or automl #####
  log_every_n_steps: 1
  logger:
    - _target_: lightning.pytorch.loggers.WandbLogger
      project: 'ltvu++'
      entity: 'team-khu'
      group: 'base'  # for grouping runs (does not mean an organization)
      job_type: ${job_type}  # debug or batch
      name: hg-${jid}  # displayname
      tags: []  # for filtering
      notes: null  # real note, a log string
      save_dir: ${runtime_outdir}  # logs will be saved under THIS/wandb
      log_model: False    # as we use custom checkpointing

dataset:
  ###### torch #####
  batch_size: ${batch_size}
  num_workers: ${num_workers}
  prefetch_factor: ${prefetch_factor}
  pin_memory: True
  persistent_workers: False  # No, our dataset is memory-bound
  ##### project-related #####
  # clips or frames should be located like: ./{clip_uid}.mp4 or ./{clip_uid}/frame_{1+frame_idx:07d}.jpg
  # for frames, frame_idxs should be 1-based
  clips_dir: '/local_datasets/ego4d_data/v2/vq2d_frames/320ss'
  official_anns_dir: '/data/datasets/ego4d_data/v2/annotations'
  flat_anns_dir: './data'
  num_frames: 32
  frame_interval: 1
  segment_size: [448, 448]  # height, width
  query_size: [448, 448]  # height, width
  query_padding: False
  query_square: True
  padding_value: 'zero'  # zero or mean

optim:
  optimizer:
    _target_: torch.optim.AdamW
    lr: ${eval:'${base_lr} / 24 * ${total_bsz}'}  # 0.0003 / 24 (official bsz) * (ours bsz)
    weight_decay: 0.005
  lr_scheduler:
    # 1000(official steps) * 24(official bsz) / (ours bsz)
    # warmup_iter: ${eval:'1000 * 24 / ${total_bsz}'}
    warmup_iter: 1000
    max_steps: ${max_steps}

loss:
  positive_threshold: .2
  logit_scale: 1.   # reciprocal of temperature
  weight_bbox_center: 1.
  weight_bbox_hw: 1.
  weight_bbox_giou: .3
  weight_prob: 100.

hydra:
  run:
    dir: ./outputs/${job_type}/${YY}-${mm}-${dd}/${jid}
