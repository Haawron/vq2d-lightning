defaults:
  - _self_
  - train: augments
  - model: vqloc
  - override hydra/job_logging: none
  - override hydra/hydra_logging: none

random_seed: 42

# resolvers should be evaluated only once
jid: ${oc.env:SLURM_JOB_ID}
job_type: ${job_type:}  # debug or batch
YY: ${now:%Y}
mm: ${now:%m}  # month
dd: ${now:%d}  # day
runtime_outdir: ${runtime_outdir:}
total_bsz: ${eval:'${dataset.batch_size} * ${oc.env:SLURM_GPUS_ON_NODE}'}
max_steps: ${eval:'${trainer.max_epochs} * 13607 // ${total_bsz} + 1'}
base_lr: 0.0003

run_type: train  # train, val, test, predict=eval


trainer:
  ##### directly affects to performance #####
  devices: 'auto'  # maximum available
  max_epochs: 106
  max_steps: ${max_steps}
  accumulate_grad_batches: 1
  precision: bf16-mixed
  gradient_clip_val: 20.
  detect_anomaly: False
  benchmark: True
  deterministic: False  # sorry
  ##### logging or automl #####
  log_every_n_steps: 1
  logger:
    - _target_: pytorch_lightning.loggers.WandbLogger
      project: 'ltvu++'
      entity: 'team-khu'
      group: 'smoke_${YY}${mm}${dd}'  # for grouping runs (does not mean an organization)
      job_type: ${job_type}  # train, eval, test, predict, debug, for filtering
      name: hg-${jid}  # displayname
      tags: []  # for filtering
      notes: null  # real note, a log string
      save_dir: ${runtime_outdir}  # logs will be saved under THIS/wandb
      log_model: False    # as we use custom checkpointing

dataset:
  ###### torch #####
  batch_size: 16
  num_workers: 3
  prefetch_factor: 2
  pin_memory: True
  persistent_workers: False  # No, our dataset is memory-bound
  ##### project-related #####
  # clips or frames should be located like: ./{clip_uid}.mp4 or ./{clip_uid}/frame_{frame_idx:07d}.jpg
  # for frames, frame_idx should be based on fps=5
  clips_dir: '/local_datasets/ego4d_data/v2/vq2d_frames/520ss'
  official_anns_dir: '/data/datasets/ego4d_data/v2/annotations'
  flat_anns_dir: './data'
  num_frames: 32
  frame_interval: 1
  segment_size: [448, 448]  # height, width
  query_size: [448, 448]  # height, width
  query_padding: False
  query_square: True
  padding_value: 'zero'  # zero or mean

optim:
  optimizer:
    _target_: torch.optim.AdamW
    lr: ${eval:'${base_lr} / 24 * ${total_bsz}'}  # 0.0003 / 24 (official bsz) * (ours bsz)
    weight_decay: 0.005
  lr_scheduler:
    # 1000(official steps) * 24(official bsz) / (ours bsz)
    # warmup_iter: ${eval:'1000 * 24 / ${total_bsz}'}
    warmup_iter: 1000
    max_steps: ${max_steps}

loss:
  positive_threshold: .2
  logit_scale: 1.   # reciprocal of temperature
  weight_bbox_center: 1.
  weight_bbox_hw: 1.
  weight_bbox_giou: .3
  weight_prob: 100.

hydra:
  run:
    dir: ./outputs/${job_type}/${YY}-${mm}-${dd}/${jid}
