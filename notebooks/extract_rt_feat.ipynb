{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/joohyun7u/project/vq2d-lightning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/joohyun7u/anaconda3/envs/vqlight3/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /data/joohyun7u/project/vq2d-lightning\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/joohyun7u/anaconda3/envs/vqlight3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_577641/2392022928.py:51: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  hydra.initialize(config_path='config', job_name='asdasdasdasdasd')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image as IPImage\n",
    "\n",
    "from ltvu.lit.model import LitModule\n",
    "from ltvu.lit.data import LitVQ2DDataModule\n",
    "\n",
    "import hydra\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from omegaconf import OmegaConf\n",
    "# from diffusers.utils import make_image_grid\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from einops import rearrange, repeat\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "def ten2pil(tensor, pad: float = 0.02, alpha = None, cmap = 'viridis'):\n",
    "    assert tensor.dim() in (2, 3)  # (H, W), (C, H, W)\n",
    "    tensor = tensor.cpu()\n",
    "    tensor -= tensor.min()\n",
    "    tensor /= tensor.max()\n",
    "\n",
    "    if tensor.dim() == 3:\n",
    "        tensor = tensor.permute(1, 2, 0)\n",
    "        assert tensor.shape[-1] == 3\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    ax = plt.Axes(fig, [pad, pad, 1. - 2* pad, 1. - 2 * pad])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    ax.imshow(tensor, aspect='equal', alpha=alpha, cmap=cmap)\n",
    "\n",
    "    plots_io = BytesIO()\n",
    "    fig.savefig(plots_io, format='jpg' if alpha is None else 'png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    img = Image.open(plots_io)\n",
    "    # plots_io.close()\n",
    "    return img\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "OmegaConf.clear_resolvers()\n",
    "OmegaConf.register_new_resolver(\"job_type\", lambda : 'debug')\n",
    "OmegaConf.register_new_resolver('runtime_outdir', lambda : 'outputs/tmp')\n",
    "OmegaConf.register_new_resolver(\"eval\", eval)\n",
    "OmegaConf.register_new_resolver(\"tuple\", lambda *args: tuple(args))\n",
    "hydra.initialize(config_path='config', job_name='asdasdasdasdasd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SLURM_GPUS_ON_NODE\"] = \"1\"  # 기본값 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ckpt = '../outputs/batch/2024-10-22/54448/epoch=78-iou=0.5046.ckpt'\n",
    "plm = LitModule.load_from_checkpoint(path_ckpt).cuda()\n",
    "\n",
    "eval_config = hydra.compose(config_name='base_jh', overrides=[\n",
    "    # f'ckpt={path_ckpt.replace('=', '\\\\=')}',\n",
    "    f'batch_size=1',\n",
    "    f'num_workers=4',\n",
    "    f'prefetch_factor=1'\n",
    "])\n",
    "pdm = LitVQ2DDataModule(eval_config)  # won't use trainer here nor batched forward pass so no need to load the eval config and plm.config is enough\n",
    "pdm.batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "normalization = kornia.enhance.Normalize(mean=MEAN, std=STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "import json\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "from pathlib import Path\n",
    "# for batch in pdm.train_dataloader():\n",
    "\n",
    "p_ann = Path('../data/vq_v2_train_anno.json')\n",
    "all_anns = json.load(p_ann.open())\n",
    "\n",
    "b,bb, bidx= 0, 0, 0\n",
    "# for bidx in range(13607):\n",
    "while True:\n",
    "    if bidx == 13607:\n",
    "        break\n",
    "    rt_ann = [ann['fno'] for ann in all_anns[bidx]['response_track']]\n",
    "    anns = all_anns[bidx]\n",
    "    bb += len(rt_ann)\n",
    "    a = 0\n",
    "    clip_uid, annotation_uid, query_set = anns['clip_uid'], anns['annotation_uid'], anns['query_set']\n",
    "    for idx, frame_idx in enumerate(rt_ann):\n",
    "        file_path = Path('../outputs/rt_pos_quries_feat') / 'train' / clip_uid / f'{clip_uid}_{frame_idx}_{annotation_uid}_{query_set}.pt'\n",
    "        if not file_path.exists():\n",
    "            print(file_path)\n",
    "            a +=1\n",
    "        else:\n",
    "            b+= 1\n",
    "    if a == 0:\n",
    "        bidx += 1\n",
    "        continue\n",
    "    print(bidx, a, 'rt_len :', len(rt_ann))\n",
    "    \n",
    "    batch = pdm.get_train_sample(idx=bidx)\n",
    "    print(rt_ann[0],rt_ann[-1], '::', batch['experiment']['multi_query']['rt_pos_idx'])\n",
    "    rt_pos_queries = batch['experiment']['multi_query']['rt_pos_queries']  # [b, #Q, c, h, w]\n",
    "    bsz = rt_pos_queries.shape[0]\n",
    "    rt_pos_queries = rearrange(rt_pos_queries, 'b q c h w -> (b q) c h w')\n",
    "    rt_pos_queries = normalization(rt_pos_queries)  # [b*#Q, c, h, w]\n",
    "    rt_pos_queries = rearrange(rt_pos_queries, '(b q) c h w -> b q c h w', b=bsz)\n",
    "    batch['rt_pos_queries'] = rt_pos_queries\n",
    "    for k, v in batch.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            batch[k] = v.cuda().float() #.to(torch.bfloat16)\n",
    "         \n",
    "    extra_args = {}\n",
    "    extra_args['sim_mode']='max'\n",
    "    extra_args['sim_thr']=0.0\n",
    "    extra_args['enable_rt_pq_threshold']= False\n",
    "    extra_args['rt_pos']=True\n",
    "    extra_args['get_intermediate_features']=True\n",
    "    plm.model = plm.model.cuda().to(torch.bfloat16)\n",
    "    output_dict, output_query = plm.model.extract_rt_feat(**batch, compute_loss=True, **extra_args)\n",
    "    for b, (clip_uid, frame_idxs, annotation_uid, query_set) in enumerate(zip(batch['clip_uid'], batch['experiment']['multi_query']['rt_pos_idx'], batch['annotation_uid'], batch['query_set'])):\n",
    "        for idx, frame_idx in enumerate(frame_idxs):\n",
    "            if frame_idx != -1:\n",
    "                p_feat_out_dir = Path('../outputs/rt_pos_quries_feat') / 'train' / clip_uid\n",
    "                p_feat_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "                file_path = p_feat_out_dir / f'{clip_uid}_{frame_idx}_{annotation_uid}_{query_set}.pt'\n",
    "                if file_path.exists():\n",
    "                    pass\n",
    "                else:\n",
    "                    print(file_path)\n",
    "                    torch.save(output_dict[b,idx,:].detach().cpu(), file_path)\n",
    "        \n",
    "print('total' , b, bb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923,\n",
       "         924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937,\n",
       "         938, 939, 940, 941]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['experiment']['multi_query']['rt_pos_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1099, 1109]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "p_ann = Path('/data/joohyun7u/project/vq2d-lightning/data/vq_v2_train_anno.json')\n",
    "all_anns = json.load(p_ann.open())\n",
    "\n",
    "b,bb, bidx= 0, 0, 0\n",
    "# for bidx in range(13607):\n",
    "\n",
    "rt_ann = [ann['fno'] for ann in all_anns[bidx]['response_track']]\n",
    "anns = all_anns[bidx]\n",
    "anns['response_track_valid_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# 대상 디렉터리 설정\n",
    "root_dir = '/data/joohyun7u/project/vq2d-lightning/outputs/rt_pos_quries_feat/train'\n",
    "i = 0\n",
    "# 모든 하위 디렉터리를 탐색하며 .pt 파일 처리\n",
    "for subdir, _, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.pt'):\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            \n",
    "            # .pt 파일 로드, detach 후 CPU로 이동\n",
    "            tensor = torch.load(file_path, map_location='cpu').detach().cpu()\n",
    "            \n",
    "            # 텐서를 같은 경로에 다시 저장\n",
    "            torch.save(tensor, file_path)\n",
    "            print(f\"{i} Processed and saved: {file_path}\")\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqlight3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
