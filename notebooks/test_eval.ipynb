{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def fix_predictions_order_and_clips(pred_file, gt_file, output_file):\n",
    "    # Load the ground truth and predictions\n",
    "    with open(gt_file, \"r\") as fp:\n",
    "        gt_annotations = json.load(fp)\n",
    "    with open(pred_file, \"r\") as fp:\n",
    "        model_predictions = json.load(fp)\n",
    "\n",
    "    # Extract the video_uid list from the ground truth\n",
    "    gt_video_uids = [v[\"video_uid\"] for v in gt_annotations[\"videos\"]]\n",
    "\n",
    "    # Extract the clips for each video from the ground truth\n",
    "    gt_clips_dict = {\n",
    "        v[\"video_uid\"]: {clip[\"clip_uid\"]: clip for clip in v[\"clips\"]}\n",
    "        for v in gt_annotations[\"videos\"]\n",
    "    }\n",
    "\n",
    "    # Extract video predictions and create a dictionary for faster lookup\n",
    "    video_predictions = model_predictions[\"results\"][\"videos\"]\n",
    "    video_predictions_dict = {v[\"video_uid\"]: v for v in video_predictions}\n",
    "\n",
    "    # Combine predictions and extra data (empty entries for missing video_uids)\n",
    "    combined_predictions = []\n",
    "\n",
    "    for uid in gt_video_uids:\n",
    "        if uid in video_predictions_dict:\n",
    "            # Get the prediction for this video\n",
    "            vpred = video_predictions_dict[uid]\n",
    "            \n",
    "            # Sort clips based on ground truth clip order\n",
    "            if \"clips\" in vpred and uid in gt_clips_dict:\n",
    "                gt_clips = gt_clips_dict[uid]\n",
    "                vpred_clips_dict = {clip[\"clip_uid\"]: clip for clip in vpred[\"clips\"]}\n",
    "                \n",
    "                # Ensure all ground truth clips are present in predictions\n",
    "                sorted_clips = []\n",
    "                for clip_uid, gt_clip in gt_clips.items():\n",
    "                    if clip_uid in vpred_clips_dict:\n",
    "                        pred_clip = vpred_clips_dict[clip_uid]\n",
    "\n",
    "                        # Ensure number of predictions matches the number of annotations\n",
    "                        num_annotations = len(gt_clip[\"annotations\"])\n",
    "                        num_predictions = len(pred_clip[\"predictions\"])\n",
    "\n",
    "                        if num_predictions < num_annotations:\n",
    "                            # Add empty predictions if there are fewer predictions than annotations\n",
    "                            for _ in range(num_predictions, num_annotations):\n",
    "                                pred_clip[\"predictions\"].append({\"query_sets\": {}})\n",
    "                        \n",
    "                        sorted_clips.append(pred_clip)\n",
    "                    else:\n",
    "                        # Add an empty clip with the same number of empty predictions as annotations\n",
    "                        sorted_clips.append({\n",
    "                            \"clip_uid\": clip_uid,\n",
    "                            \"predictions\": [{\"query_sets\": {}} for _ in range(len(gt_clip[\"annotations\"]))]\n",
    "                        })\n",
    "                vpred[\"clips\"] = sorted_clips\n",
    "            else:\n",
    "                # If no clips exist in predictions, create empty clips for the video\n",
    "                vpred[\"clips\"] = [\n",
    "                    {\n",
    "                        \"clip_uid\": clip_uid,\n",
    "                        \"predictions\": [{\"query_sets\": {}} for _ in range(len(gt_clips_dict[uid][clip_uid][\"annotations\"]))]\n",
    "                    }\n",
    "                    for clip_uid in gt_clips_dict[uid]\n",
    "                ]\n",
    "\n",
    "            combined_predictions.append(vpred)\n",
    "        else:\n",
    "            # Add missing video entries as empty predictions\n",
    "            combined_predictions.append({\n",
    "                \"video_uid\": uid, \n",
    "                \"split\": \"test\", \n",
    "                \"clips\": [\n",
    "                    {\n",
    "                        \"clip_uid\": clip_uid,\n",
    "                        \"predictions\": [{\"query_sets\": {}} for _ in range(len(gt_clips_dict[uid][clip_uid][\"annotations\"]))]\n",
    "                    }\n",
    "                    for clip_uid in gt_clips_dict[uid]\n",
    "                ]\n",
    "            })\n",
    "\n",
    "    # Update the predictions in the model_predictions object\n",
    "    model_predictions[\"results\"][\"videos\"] = combined_predictions\n",
    "\n",
    "    # Write the updated predictions to a new file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(model_predictions, f)\n",
    "\n",
    "    print(f\"Updated predictions saved to: {output_file}\")\n",
    "\n",
    "# Paths to your input and output files\n",
    "pred_file = '../outputs/batch/2024-10-13/52661/test_predictions.json'\n",
    "gt_file = '/data/dataset/ego4d_temp/ego4d_data/v2/annotations/vq_test_unannotated.json'\n",
    "output_file = Path(pred_file).with_name('test_predictions_sorted.json')\n",
    "\n",
    "# Call the function to fix the order and clips\n",
    "fix_predictions_order_and_clips(pred_file, gt_file, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqlight3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
