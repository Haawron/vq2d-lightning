{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video_uid': '56a3b49c-6979-4253-9ff5-2733c3e2f229',\n",
       " 'clip_uid': '3fe39989-54ed-4a3a-bea4-88be05ff8df7',\n",
       " 'annotation_uid': 'c64628fc-f163-4eb6-b7b1-f89b35cdcf54',\n",
       " 'query_set': '1',\n",
       " 'clip_fps': 5.0,\n",
       " 'clip_duration': 300.00000000000006,\n",
       " 'original_width': 1440,\n",
       " 'original_height': 1080,\n",
       " 'query_frame': 1190,\n",
       " 'object_title': 'mr krips',\n",
       " 'visual_crop': {'fno': 1198,\n",
       "  'x': 742.08,\n",
       "  'y': 587.21,\n",
       "  'w': 78.35,\n",
       "  'h': 121.73},\n",
       " 'response_track_valid_range': [1099, 1109],\n",
       " 'response_track': [{'fno': 1099,\n",
       "   'x': 929.68,\n",
       "   'y': 540.42,\n",
       "   'w': 11.2,\n",
       "   'h': 41.4},\n",
       "  {'fno': 1100, 'x': 917.51, 'y': 542.37, 'w': 12.18, 'h': 43.34},\n",
       "  {'fno': 1101, 'x': 910.69, 'y': 546.26, 'w': 12.18, 'h': 48.12},\n",
       "  {'fno': 1102, 'x': 930.17, 'y': 544.32, 'w': 18.51, 'h': 51.9},\n",
       "  {'fno': 1103, 'x': 966.7, 'y': 547.33, 'w': 17.23, 'h': 56.89},\n",
       "  {'fno': 1104, 'x': 1004.2, 'y': 557.32, 'w': 21.08, 'h': 62.97},\n",
       "  {'fno': 1105, 'x': 1032.44, 'y': 549.99, 'w': 24.4, 'h': 74.68},\n",
       "  {'fno': 1106, 'x': 1066.68, 'y': 542.66, 'w': 34.13, 'h': 87.75},\n",
       "  {'fno': 1107, 'x': 1123.92, 'y': 554.75, 'w': 40.58, 'h': 103.47},\n",
       "  {'fno': 1108, 'x': 1222.56, 'y': 568.19, 'w': 52.98, 'h': 118.77},\n",
       "  {'fno': 1109, 'x': 1352.76, 'y': 591.14, 'w': 76.09, 'h': 141.48}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 'train'\n",
    "\n",
    "all_anns = json.load(open(f'../data/vq_v2_{split}_anno.json'))\n",
    "all_anns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 5264/13607 [06:33<10:22, 13.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681.03 1105.7 681.03 1107.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot write empty image as JPEG",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     cropped \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mcrop((x1, y1, x2, y2))\n\u001b[0;32m---> 40\u001b[0m \u001b[43mcropped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aidx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     42\u001b[0m     display(cropped)\n",
      "File \u001b[0;32m/data/soyeonhong/anaconda3/envs/vq2d-lightning/lib/python3.12/site-packages/PIL/Image.py:2568\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2565\u001b[0m     fp \u001b[38;5;241m=\u001b[39m cast(IO[\u001b[38;5;28mbytes\u001b[39m], fp)\n\u001b[1;32m   2567\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2568\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   2570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[0;32m/data/soyeonhong/anaconda3/envs/vq2d-lightning/lib/python3.12/site-packages/PIL/JpegImagePlugin.py:636\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m im\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    635\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot write empty image as JPEG\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 636\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    639\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m RAWMODE[im\u001b[38;5;241m.\u001b[39mmode]\n",
      "\u001b[0;31mValueError\u001b[0m: cannot write empty image as JPEG"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from decord import VideoReader\n",
    "# /local_datasets/ego4d_data/v2/vq2d_frames/520ss\n",
    "p_clips_dir = Path('/data/datasets/ego4d_data/v2/clips')\n",
    "p_crop_out_dir = Path('../outputs/rt_pos_queries') / split\n",
    "\n",
    "for aidx, ann in enumerate(tqdm(all_anns)):\n",
    "    clip_uid = ann['clip_uid']\n",
    "    qset_uuid = f\"{ann['annotation_uid']}_{ann['query_set']}\"\n",
    "    rt = ann['response_track']\n",
    "    ow, oh = ann['original_width'], ann['original_height']\n",
    "    frame_idxs = [f['fno'] for f in rt]\n",
    "\n",
    "    p_obj_dir = p_crop_out_dir / clip_uid\n",
    "    p_obj_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    p_clip = p_clips_dir / f'{clip_uid}.mp4'\n",
    "    vr = VideoReader(str(p_clip))\n",
    "    \n",
    "    for idx, frame_idx in enumerate(frame_idxs):\n",
    "        p_out = p_obj_dir / f'{clip_uid}_{frame_idx}_{qset_uuid}.jpg'\n",
    "        \n",
    "        if p_out.exists():\n",
    "            continue\n",
    "        \n",
    "        frame = vr[min(6*frame_idx, len(vr)-1)].asnumpy()\n",
    "        w, h = frame.shape[1], frame.shape[0]\n",
    "        x1, y1, x2, y2 = rt[idx]['x'] / ow * w, rt[idx]['y'] / oh * h, (rt[idx]['x'] + rt[idx]['w']) / ow * w, (rt[idx]['y'] + rt[idx]['h']) / oh * h\n",
    "        \n",
    "        img = Image.fromarray(frame)\n",
    "        \n",
    "        if x2 - x1 == 0 or y2 - y1 == 0:\n",
    "            print(f\"Empty crop: {p_out}\")\n",
    "            cropped = img\n",
    "        else:\n",
    "            cropped = img.crop((x1, y1, x2, y2))\n",
    "        \n",
    "        cropped.save(p_out)\n",
    "        if aidx % 500 == 0:\n",
    "            display(cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vq2d-lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
